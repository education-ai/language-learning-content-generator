{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Generate Learning Content for Language Teaching Materials\n",
    "Prototype for Experiments\n",
    "\"\"\"\n",
    "__author__ = [\"Leo S. Rüdian\"]\n",
    "__copyright__ = \"2024, Rüdian\"\n",
    "__credits__ = [\"Leo S. Rüdian\"]\n",
    "__license__ = \"CC BY-NC-SA\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = [\"Leo S. Rüdian\"]\n",
    "__email__ =[\"ruediasy@informatik.hu-berlin.de\"]\n",
    "__status__ = \"Prototype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e7a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "import random\n",
    "import json\n",
    "from os.path import exists\n",
    "import hashlib\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "import requests\n",
    "import shutil # to save it locally\n",
    "import time\n",
    "\n",
    "openai.api_key = '[ADD YOUR KEY]'\n",
    "'''\n",
    "OpenAI API to access gpt-3.5-turbo\n",
    "'''\n",
    "def getChatGPT(txt,cat=0,force=False):\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=[\n",
    "                {\"role\": \"user\", \"content\": txt},\n",
    "            ]\n",
    "        )\n",
    "        return resp['choices'][0]['message']['content']\n",
    "    \n",
    "    except:\n",
    "        print('GPT ERROR')\n",
    "        # try again\n",
    "        time.sleep(5)\n",
    "        return getChatGPT(txt,cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f92f4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions\n",
    "'''\n",
    "\n",
    "'''\n",
    "Convert textual response to list\n",
    "'''\n",
    "def response2list(t):\n",
    "    temp = t.split(\"\\n\")\n",
    "    lst = []\n",
    "    for i in temp:\n",
    "        if i != '' and ' ' in i:\n",
    "            c = i.index(' ')\n",
    "            add = i[c+1:].replace('- ','')\n",
    "            lst.append(add)\n",
    "    return lst\n",
    "\n",
    "'''\n",
    "Convert two-dimensional textual response to list\n",
    "'''\n",
    "def response2dim2list(t):\n",
    "    #t = t.replace('- ','')\n",
    "    temp = t.split(\"\\n\")\n",
    "    lst = {}\n",
    "    for i in temp:\n",
    "        if i != '' and ' ' in i:\n",
    "            c = i.index(':')\n",
    "            add = i[c+1:].strip().split(',')\n",
    "            if add[0:1]=='-': add = add[1:]\n",
    "            lst[i[0:c].strip()]=(add)\n",
    "    return lst\n",
    "\n",
    "'''\n",
    "Reverse the list from key to value\n",
    "'''\n",
    "def voc_reverse(voc_combine):\n",
    "    voc_combine_rev = {}\n",
    "    for i in voc_combine:\n",
    "        for j in voc_combine[i]:\n",
    "            #print(j)\n",
    "            w = j.strip()\n",
    "            if len(w) < 20:\n",
    "                if not w in voc_combine_rev:\n",
    "                    voc_combine_rev[w]=[i]\n",
    "                else:\n",
    "                    voc_combine_rev[w].append(i)\n",
    "    return voc_combine_rev\n",
    "\n",
    "'''\n",
    "Return intersection of lists\n",
    "'''\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "    \n",
    "'''\n",
    "Select vocab that fulfills constraints\n",
    "''' \n",
    "def selectVocab():\n",
    "    global voc_combine_rev\n",
    "    # identify a\n",
    "    voc_combine_rev_wk = voc_combine_rev.copy()\n",
    "    candidate_a = {}\n",
    "    for i in voc_combine_rev:\n",
    "        if len(voc_combine_rev[i]) >= 4:\n",
    "            candidate_a[i] = voc_combine_rev_wk[i]\n",
    "\n",
    "    # check whether noun exist at least 4 times, and remove otherwise\n",
    "    for i in voc_combine:\n",
    "        cnter = 0\n",
    "        for j in candidate_a:\n",
    "            if i in voc_combine_rev_wk[j]:\n",
    "                cnter += 1\n",
    "        if cnter < 2:\n",
    "            for j in candidate_a:\n",
    "                if i in candidate_a[j]:\n",
    "                    candidate_a[j].remove(i)\n",
    "                \n",
    "    # remove finally if exists less than 4 items\n",
    "    candidate_a_new = {}\n",
    "    for i in candidate_a:\n",
    "        if len(candidate_a[i]) >= 2:\n",
    "            candidate_a_new[i] = candidate_a[i]\n",
    "    candidate_a = candidate_a_new\n",
    "    return candidate_a\n",
    "\n",
    "'''\n",
    "Select candidate of interest\n",
    "'''\n",
    "def select2er():\n",
    "    global voc_combine_rev\n",
    "    candicate_intersect = []\n",
    "    for i in voc_combine_rev:\n",
    "        for j in voc_combine_rev:\n",
    "            if j!=i:\n",
    "                intersect = intersection(voc_combine_rev[i],voc_combine_rev[j]) \n",
    "                if len(intersect)>= 2:\n",
    "                    random.shuffle(intersect)\n",
    "                    candicate_intersect.append([i,j,intersect])\n",
    "                    \n",
    "    return candicate_intersect\n",
    "\n",
    "'''\n",
    "Select random item\n",
    "''' \n",
    "def selectrandom():\n",
    "    global candicate_intersect,candicate_intersect_filtered,candidate_D_a,candidate_D_d,maxrand\n",
    "    candidate_D_d = random.choice(candicate_intersect)\n",
    "    candidate_D_d = [candidate_D_d[0],candidate_D_d[1],candidate_D_d[2]]\n",
    "\n",
    "    # filter by D_d nouns\n",
    "    candicate_intersect_filtered = []\n",
    "    for i in candicate_intersect:\n",
    "        if candidate_D_d[0] in [i[0],i[1]] or candidate_D_d[1] in [i[0],i[1]]:\n",
    "            isin = False\n",
    "            for j in candidate_D_d[2]:\n",
    "                if j in i[2]:\n",
    "                    isin = True\n",
    "            if not isin:\n",
    "                candicate_intersect_filtered.append(i)\n",
    "    try:\n",
    "        candidate_D_a = random.choice(candicate_intersect_filtered)\n",
    "        while candidate_D_a == candidate_D_d:# or :\n",
    "            candidate_D_a = random.choice(candicate_intersect_filtered)\n",
    "        candidate_D_a = [candidate_D_a[0],candidate_D_a[1],candidate_D_a[2]]\n",
    "        return candidate_D_a,candidate_D_d\n",
    "    except:\n",
    "        maxrand -= 1\n",
    "        if maxrand > 0:\n",
    "            return selectrandom()\n",
    "\n",
    "'''\n",
    "Select list [A,B,C,D,E,F,G,H,a,b,c,d] of possible candidates\n",
    "'''\n",
    "def makevoc(candidate_D_a,candidate_D_d):\n",
    "    global voc_combine_rev, voc_noun\n",
    "    if candidate_D_a[0] == candidate_D_d[0]:\n",
    "        shared_verb = candidate_D_a[0]\n",
    "        b = candidate_D_d[1]\n",
    "        c = candidate_D_a[1]\n",
    "    elif candidate_D_a[1] == candidate_D_d[1]:\n",
    "        shared_verb = candidate_D_a[1]\n",
    "        b = candidate_D_d[0]\n",
    "        c = candidate_D_a[0]\n",
    "    elif candidate_D_a[0] == candidate_D_d[1]:\n",
    "        shared_verb = candidate_D_a[0]\n",
    "        b = candidate_D_d[0]\n",
    "        c = candidate_D_a[1]\n",
    "    else:\n",
    "        shared_verb = candidate_D_a[1]\n",
    "        b = candidate_D_d[1]\n",
    "        c = candidate_D_a[0]\n",
    "    a = shared_verb\n",
    "\n",
    "    A = candidate_D_d[2][0]\n",
    "    B = candidate_D_d[2][1]\n",
    "    C = candidate_D_a[2][0]\n",
    "    D = candidate_D_a[2][1]\n",
    "\n",
    "    # find d, which is not a,b,c, but compatible with D\n",
    "    candidate_d = []\n",
    "    for i in voc_combine_rev:\n",
    "        if i != a and i != b and i != c:\n",
    "            if D in voc_combine_rev[i]:\n",
    "                candidate_d.append(i)\n",
    "    random.shuffle(candidate_d)\n",
    "    #print(candidate_d)\n",
    "    d = candidate_d[0]\n",
    "    \n",
    "    # identify remaining nouns\n",
    "    voc_noun_rest = voc_noun.copy()\n",
    "    if A in voc_noun_rest: voc_noun_rest.remove(A)\n",
    "    if B in voc_noun_rest: voc_noun_rest.remove(B)\n",
    "    if C in voc_noun_rest: voc_noun_rest.remove(C)\n",
    "    if D in voc_noun_rest: voc_noun_rest.remove(D)\n",
    "    #print(voc_noun_rest)\n",
    "    random.shuffle(voc_noun_rest)\n",
    "    E = voc_noun_rest[0]\n",
    "    F = voc_noun_rest[1]\n",
    "    G = voc_noun_rest[2]\n",
    "    H = voc_noun_rest[3]\n",
    "    return [A,B,C,D,E,F,G,H,a,b,c,d]\n",
    "\n",
    "'''\n",
    "Store course content in course/\n",
    "'''\n",
    "def storecourse(data,vocab):\n",
    "    try:\n",
    "        coursefile = \"course/\"+vocab+\".json\"\n",
    "        nr = 1\n",
    "        while exists(coursefile):\n",
    "            coursefile = \"course/\"+vocab+'#'+str(nr)+\".json\"\n",
    "            nr += 1\n",
    "\n",
    "        with open(coursefile, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(data))\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        print('Storage ERROR')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab5e93",
   "metadata": {},
   "source": [
    "## Generate Learning Content of 200 Topics (Random Surfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = '[ADD KEYWORD]'\n",
    "number_of_microlearning_contents = 200\n",
    "debug = True\n",
    "random_voc = [vocab]\n",
    "\n",
    "for i in range(number_of_microlearning_contents):\n",
    "    try:\n",
    "        if debug: print(vocab)\n",
    "        voc_sent = {}\n",
    "        voc_verb = response2list(getChatGPT(\"Erstelle eine Liste von 20 Verben zum Thema \\\"\"+vocab+\"\\\". Sprachlevel A1\",1))\n",
    "        voc_noun = response2list(getChatGPT(\"Erstelle eine Liste von 20 Substantiven zum Thema \\\"\"+vocab+\"\\\". Sprachlevel A1\",2))\n",
    "\n",
    "        voc_combine = (getChatGPT(\"Kombiniere Wörter \\\"\"+','.join(voc_noun)+\"\\\" mit \\\"\"+','.join(voc_verb)+\"\\\"\"+\n",
    "                                              \". Die Liste soll nur das Substantiv und mindestens 4 Verben in einer Zeile enthalten. Verwende keine Nummern und keine Aufzählung.\", 3))\n",
    "        voc_combine = response2dim2list(voc_combine)\n",
    "        if debug:print('Vocab downloaded')\n",
    "            \n",
    "        voc_combine_rev = voc_reverse(voc_combine)\n",
    "\n",
    "        # 1st constraint: [ABCD]+[a]\n",
    "        candidate_a = selectVocab()\n",
    "        if debug:print('constraint 1 erfüllt: ', len(candidate_a)>0)\n",
    "        if debug:print(candidate_a)\n",
    "\n",
    "        # 2nd constraint: [AC]+[a], [BD]+[a], [AB]+[b], [CD]+[c]\n",
    "        candicate_intersect = select2er()\n",
    "        #print('candicate_intersect',candicate_intersect)\n",
    "        maxrand = 10\n",
    "        candidate_D_a, candidate_D_d = selectrandom()\n",
    "\n",
    "        # get vocab\n",
    "        A,B,C,D,E,F,G,H,a,b,c,d = makevoc(candidate_D_a,candidate_D_d)\n",
    "        random_voc.extend([A,B,C,D,E,F,G,H])\n",
    "\n",
    "        if debug:print('A,B,C,D,E,F,G,H,a,b,c,d ready')\n",
    "        \n",
    "        # create sentences of 2 words\n",
    "        Aa = (getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([A,a])+\"\\ enthält. Der Satz darf maximal 6 Wörter enthalten. Verwende bei Substantiven einen Artikel. Sprachlevel A1\",6)).strip()\n",
    "        Ba = (getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([B,a])+\"\\ enthält. Der Satz darf maximal 6 Wörter enthalten. Verwende bei Substantiven einen Artikel. Sprachlevel A1\",6)).strip()\n",
    "        Ca = (getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([C,a])+\"\\ enthält. Der Satz darf maximal 6 Wörter enthalten. Verwende bei Substantiven einen Artikel. Sprachlevel A1\",6)).strip()\n",
    "        Da = (getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([D,a])+\"\\ enthält. Der Satz darf maximal 6 Wörter enthalten. Verwende bei Substantiven einen Artikel. Sprachlevel A1\",6)).strip()\n",
    "        if debug:print('4 2er sentences created')\n",
    "            \n",
    "        # create sentences of 3 words\n",
    "        ABb =(getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([A,B,b])+\"\\\" enthält. Der Satz darf maximal 12 Wörter enthalten. Sprachlevel A1. Jeder Satz muss \\\"\"+','.join([A,B,b])+\"\\ enthalten. Sprachlevel A1\",4)).strip()\n",
    "        CDc =(getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([C,D,c])+\"\\\" enthält. Der Satz darf maximal 12 Wörter enthalten. Sprachlevel A1. Jeder Satz muss \\\"\"+','.join([C,D,c])+\"\\ enthalten. Sprachlevel A1\",4)).strip()\n",
    "        ACa =(getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([A,C,a])+\"\\\" enthält. Der Satz darf maximal 12 Wörter enthalten. Sprachlevel A1. Jeder Satz muss \\\"\"+','.join([A,C,a])+\"\\ enthalten. Sprachlevel A1\",4)).strip()\n",
    "        BDa =(getChatGPT(\"Erstelle einen Satz, der die Wörter \\\"\"+','.join([B,D,a])+\"\\\" enthält. Der Satz darf maximal 12 Wörter enthalten. Sprachlevel A1. Jeder Satz muss \\\"\"+','.join([B,D,a])+\"\\ enthalten. Sprachlevel A1\",4)).strip()\n",
    "        if debug:print('4 3er sentences created')\n",
    "\n",
    "        voc_txt = (getChatGPT(\"Erstelle einen zusammenhängenden Text aus 10 Sätzen. Nutze folgende Wörter: \\\"\"+','.join([A,B,C,D,E,F,G,H,a,b,c,d])+\"\\\". Jeder Satz darf maximal 7 Wörter enthalten. Sprachlevel A1. Jeder Satz in einer Zeile. Verwende keine Zahlen.\",5)).strip().split(\"\\n\")\n",
    "        if debug:print('Text created')\n",
    "\n",
    "        # store all data\n",
    "        data = {\n",
    "            'A':A,'B':B,'C':C,'D':D,'E':E,'F':F,'G':G,'H':H,\n",
    "            'a':a,'b':b,'c':c,'d':d,\n",
    "            'Aa':Aa,'Ba':Ba,'Ca':Ca,'Da':Da,\n",
    "            'ABb':ABb,'CDc':CDc,'ACa':ACa,'BDa':BDa,\n",
    "            'txt':voc_txt\n",
    "        }\n",
    "\n",
    "        if storecourse(data,vocab) and debug: print('file stored')\n",
    "\n",
    "        if debug: print()\n",
    "\n",
    "        # select random vocab for next round (random surfer model)\n",
    "        vocab = random.choice(voc_noun) # or [A,B,C,D,E,F,G,H]\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print('Restart')\n",
    "        vocab = random.choice(random_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee01fc1",
   "metadata": {},
   "source": [
    "## Generate Course Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl \n",
    "auth_key = '[DEEPL KEY]'\n",
    "translator = deepl.Translator(auth_key) \n",
    "\n",
    "'''\n",
    "translates texts from German to English to create queries using DEEPL\n",
    "'''\n",
    "def translatetoenglish(txt,force=False):\n",
    "    if txt == '': return ''\n",
    "    transfile = 'translate/'+str(hashlib.sha256(txt.encode('utf-8')).hexdigest())+'.json'\n",
    "    if exists(transfile) and not force:\n",
    "        json_data = open(transfile)\n",
    "        data = json.load(json_data)\n",
    "        json_data.close()\n",
    "        return data['to']\n",
    "    \n",
    "    global translator\n",
    "    target_language = 'EN-US'\n",
    "    source_language = 'DE'\n",
    "    result = translator.translate_text(txt, target_lang=target_language, source_lang=source_language) \n",
    "    \n",
    "    with open(transfile, 'w') as f:\n",
    "        json.dump({'from':txt,'to':result.text}, f)\n",
    "        \n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "18c0caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OpenAI API to generate DALL·E2 images\n",
    "'''\n",
    "def getAIimage(query, forcenew=False):\n",
    "    query = query.lower()\n",
    "    imgfile = 'image/'+query+'.jpg'\n",
    "    try:\n",
    "        response = openai.Image.create(\n",
    "          prompt=query,\n",
    "          n=1,\n",
    "          size=\"256x256\"\n",
    "        )\n",
    "        image_url = response['data'][0]['url']\n",
    "        storeimgdirectAI(query,image_url)\n",
    "        return image_url\n",
    "    except:\n",
    "        print('ERROR '+query)\n",
    "        return None\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3958b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate Images of 200 Courses\n",
    "'''\n",
    "with open('res.txt', 'r') as f:\n",
    "    courses_read = f.read()\n",
    "  \n",
    "cnt = 200\n",
    "courses = courses_read.split(\"\\n\")\n",
    "courses.pop(0)\n",
    "\n",
    "for i in courses:\n",
    "    x = i.split('|')\n",
    "    \n",
    "    # all validated as correct and appropriate\n",
    "    if len(x)>=6 and int(x[1]) == 1 and int(x[2]) == 1 and int(x[3]) == 1 and int(x[4]) == 1 and int(x[5]) == 1 and int(x[6]) == 1 and not exists('course_img_ai/'+x[0]):\n",
    "        print(x[0])\n",
    "        if cnt > 0:\n",
    "            dat = x[0]\n",
    "            with open('course/'+dat, 'r') as f: course = json.load(f)\n",
    "\n",
    "            img = {}\n",
    "            # get word images\n",
    "            for el in ['A','B','C','D','E','F','G','H','a','b','c','d']:\n",
    "                el_en = translatetoenglish(course[el])\n",
    "                if getAIimage(el_en): #False and \n",
    "                    img[el] = el_en.lower()    \n",
    "\n",
    "            for el in ['Aa','Ba','Ca','Da','ABb','CDc','ACa','BDa']:\n",
    "                if course[el] != False:\n",
    "                    \n",
    "                    sent = translatetoenglish(course[el])\n",
    "                    if getAIimage(sent):\n",
    "                        img[el] = sent.lower().replace('\\'','').replace('?','')\n",
    "\n",
    "            txt_img = []\n",
    "            for sentence in course['txt']:\n",
    "                sent = translatetoenglish(sentence)\n",
    "                if getAIimage(sent):\n",
    "                    txt_img.append(sent.lower().replace('\\'','').replace('?',''))\n",
    "                    \n",
    "                else:\n",
    "                    txt_img.append(None)\n",
    "            img['txt'] = txt_img       \n",
    "            course['img']=img\n",
    "            \n",
    "            # = 30 images per course\n",
    "            with open('image/'+dat, \"w\") as outfile:\n",
    "                outfile.write(json.dumps(course))\n",
    "\n",
    "            cnt -= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
